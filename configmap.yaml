---
apiVersion: v1
data:
  AIRFLOW__CORE__EXECUTOR: LocalExecutor
  AIRFLOW__CORE__FERNET_KEY: "{{ .Values.fernet }}"
  AIRFLOW__CORE__SQL_ALCHEMY_CONN: "{{ .Values.db }}"
  AIRFLOW__CORE__DAGS_FOLDER: /airflow/dags
  AIRFLOW__CORE__PARALLELISM: "100"
  AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE: "0"
  DD_ENV: "{{ .Values.environment }}"
  DD_VERSION: "{{ .Values.image.tag }}"
  DD_SERVICE: "{{ .Values.serviceName }}-worker"
  AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
  AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: s3_logs
  AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "{{ .Values.s3.bucket }}"
  AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: "False"
  AIRFLOW__CORE__ENABLE_XCOM_PICKLING: "True"
  AIRFLOW__SMTP__SMTP_STARTTLS: "True"
  AIRFLOW__SMTP__SMTP_SSL: "False"
  AIRFLOW__SMTP__SMTP_MAIL_FROM: "airflow.analytics@.com.br"
  AIRFLOW__SMTP__SMTP_PORT: "587"
  AIRFLOW__SMTP__SMTP_HOST: "email-smtp.sa-east-1.amazonaws.com"
  AIRFLOW__EMAIL__EMAIL_CONN_ID: smtp_default
  AIRFLOW__EMAIL__EMAIL_BACKEND: airflow.utils.email.send_email_smtp
  AIRFLOW__SMTP__SMTP_USER: ""
  AIRFLOW__SMTP__SMTP_PASSWORD: ''
  AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL: "1800"
  AIRFLOW__WEBSERVER__BASE_URL: "https://airflow..io/"
  AIRFLOW__CLI__ENDPOINT_URL: "https://airflow..io/"
  AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "300"
  AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: "30"
kind: ConfigMap
metadata:
  name: 'airflow-worker'
---
apiVersion: v1
data:
  airflow.cfg: |-
    [core]
    # The folder where your airflow pipelines live, most likely a
    # subfolder in a code repository. This path must be absolute.
    dags_folder = /airflow/dags
kind: ConfigMap
metadata:
  name: '{{ include "airflow-data.fullname" . }}-config'